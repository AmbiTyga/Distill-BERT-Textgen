# Distill-BERT-Textgen
Research code for ACL 2020 paper: "[Distilling Knowledge Learned in BERT for Text Generation](https://arxiv.org/abs/1911.03829)".

![Overview of UNITER](https://convaisharables.blob.core.windows.net/distill-bert-textgen/overview.png)

Coming soon!


## Citation
If you find this work helpful to your research, please consider citing:
```
@inproceedings{chen2020distilling,
  title={Distilling Knowledge Learned in BERT for Text Generation},
  author={Chen, Yen-Chun and Gan, Zhe and Cheng, Yu and Liu, Jingzhou and Liu, Jingjing},
  booktitle={ACL},
  year={2020}
}
```
