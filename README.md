# Distill-BERT-Textgen
Research code for ACL 2020 paper: "[Distill Knowledge Learned in BERT for Text Generation](https://arxiv.org/abs/1911.03829)".

Coming soon!


## Citation
If you find this work helpful to your research, please consider citing:
```
@inproceedings{chen2020distilling,
  title={Distilling Knowledge Learned in BERT for Text Generation},
  author={Chen, Yen-Chun and Gan, Zhe and Cheng, Yu and Liu, Jingzhou and Liu, Jingjing},
  booktitle={ACL},
  year={2020}
}
```
